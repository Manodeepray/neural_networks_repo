{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:34.178152Z","iopub.execute_input":"2025-03-17T20:20:34.178503Z","iopub.status.idle":"2025-03-17T20:20:36.410432Z","shell.execute_reply.started":"2025-03-17T20:20:34.178479Z","shell.execute_reply":"2025-03-17T20:20:36.409028Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Attention","metadata":{}},{"cell_type":"markdown","source":"#### Self-Attention","metadata":{}},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n\n    def __init__(self , d_model = 2,\n                row_dim = 0,\n                col_dim = 1):\n\n        super().__init__()\n\n        # weight matrices\n        \n        self.W_q = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False) \n        self.W_k = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False)\n        \n        self.W_v = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False)\n\n        self.row_dim = row_dim\n        self.col_dim = col_dim\n        self.d_model = d_model\n\n\n    \n    def forward(self , token_encodings):\n        q = self.W_q(token_encodings)\n        k = self.W_k(token_encodings)\n        v = self.W_v(token_encodings)\n        \n        sims = torch.matmul(q , k.transpose(dim0 = self.row_dim,\n                                           dim1 = self.col_dim))\n        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n        # scaled_sims = sims / torch.tensor(self.d_model**0.5)\n\n        attention_percents = F.softmax(scaled_sims , dim = self.col_dim)\n        attention_scores = torch.matmul(attention_percents , v)\n\n        return attention_scores\n\n    \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.412152Z","iopub.execute_input":"2025-03-17T20:20:36.412709Z","iopub.status.idle":"2025-03-17T20:20:36.420624Z","shell.execute_reply.started":"2025-03-17T20:20:36.412679Z","shell.execute_reply":"2025-03-17T20:20:36.419380Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"encodings_matrix = torch.tensor([[1.16, 0.23],[0.57, 1.36],[4.41, -2.16]])\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.422728Z","iopub.execute_input":"2025-03-17T20:20:36.423059Z","iopub.status.idle":"2025-03-17T20:20:36.461573Z","shell.execute_reply.started":"2025-03-17T20:20:36.423033Z","shell.execute_reply":"2025-03-17T20:20:36.460227Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7eb0ce34abf0>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"self_attention = SelfAttention(d_model = 2,\n                              row_dim = 0,\n                              col_dim = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.463578Z","iopub.execute_input":"2025-03-17T20:20:36.464067Z","iopub.status.idle":"2025-03-17T20:20:36.487556Z","shell.execute_reply.started":"2025-03-17T20:20:36.464022Z","shell.execute_reply":"2025-03-17T20:20:36.486400Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"self_attention(encodings_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.488795Z","iopub.execute_input":"2025-03-17T20:20:36.489321Z","iopub.status.idle":"2025-03-17T20:20:36.617596Z","shell.execute_reply.started":"2025-03-17T20:20:36.489278Z","shell.execute_reply":"2025-03-17T20:20:36.616560Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0100, 1.0641],\n        [0.2040, 0.7057],\n        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"for name, param in self_attention.named_parameters():\n    print(f\"Parameter name: {name}\")\n    print(f\"Parameter values:\\n{param.data}\")\n    print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.618820Z","iopub.execute_input":"2025-03-17T20:20:36.619283Z","iopub.status.idle":"2025-03-17T20:20:36.630454Z","shell.execute_reply.started":"2025-03-17T20:20:36.619250Z","shell.execute_reply":"2025-03-17T20:20:36.629125Z"}},"outputs":[{"name":"stdout","text":"Parameter name: W_q.weight\nParameter values:\ntensor([[ 0.5406,  0.5869],\n        [-0.1657,  0.6496]])\n------------------------------\nParameter name: W_k.weight\nParameter values:\ntensor([[-0.1549,  0.1427],\n        [-0.3443,  0.4153]])\n------------------------------\nParameter name: W_v.weight\nParameter values:\ntensor([[ 0.6233, -0.5188],\n        [ 0.6146,  0.1323]])\n------------------------------\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Get the state dictionary\nstate_dict = self_attention.state_dict()\n\n# Print the state dictionary\nfor key, value in state_dict.items():\n    print(f\"Key: {key}\")\n    print(f\"Values:\\n{value}\")\n    print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.631340Z","iopub.execute_input":"2025-03-17T20:20:36.631686Z","iopub.status.idle":"2025-03-17T20:20:36.649064Z","shell.execute_reply.started":"2025-03-17T20:20:36.631658Z","shell.execute_reply":"2025-03-17T20:20:36.647693Z"}},"outputs":[{"name":"stdout","text":"Key: W_q.weight\nValues:\ntensor([[ 0.5406,  0.5869],\n        [-0.1657,  0.6496]])\n------------------------------\nKey: W_k.weight\nValues:\ntensor([[-0.1549,  0.1427],\n        [-0.3443,  0.4153]])\n------------------------------\nKey: W_v.weight\nValues:\ntensor([[ 0.6233, -0.5188],\n        [ 0.6146,  0.1323]])\n------------------------------\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"state_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.652122Z","iopub.execute_input":"2025-03-17T20:20:36.652516Z","iopub.status.idle":"2025-03-17T20:20:36.671061Z","shell.execute_reply.started":"2025-03-17T20:20:36.652488Z","shell.execute_reply":"2025-03-17T20:20:36.669955Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('W_q.weight',\n              tensor([[ 0.5406,  0.5869],\n                      [-0.1657,  0.6496]])),\n             ('W_k.weight',\n              tensor([[-0.1549,  0.1427],\n                      [-0.3443,  0.4153]])),\n             ('W_v.weight',\n              tensor([[ 0.6233, -0.5188],\n                      [ 0.6146,  0.1323]]))])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"self_attention.W_q.weight.transpose(0,1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.672433Z","iopub.execute_input":"2025-03-17T20:20:36.672880Z","iopub.status.idle":"2025-03-17T20:20:36.699875Z","shell.execute_reply.started":"2025-03-17T20:20:36.672816Z","shell.execute_reply":"2025-03-17T20:20:36.698311Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.5406, -0.1657],\n        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"self_attention.W_k.weight.transpose(0,1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.701112Z","iopub.execute_input":"2025-03-17T20:20:36.701521Z","iopub.status.idle":"2025-03-17T20:20:36.710044Z","shell.execute_reply.started":"2025-03-17T20:20:36.701482Z","shell.execute_reply":"2025-03-17T20:20:36.708628Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.1549, -0.3443],\n        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"self_attention.W_v.weight.transpose(0,1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T20:20:36.711454Z","iopub.execute_input":"2025-03-17T20:20:36.711940Z","iopub.status.idle":"2025-03-17T20:20:36.731177Z","shell.execute_reply.started":"2025-03-17T20:20:36.711899Z","shell.execute_reply":"2025-03-17T20:20:36.729943Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.6233,  0.6146],\n        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"#### Masked Self-Attention","metadata":{}},{"cell_type":"code","source":"class MaskedSelfAttention(nn.Module):\n\n    def __init__(self , d_model = 2,\n                row_dim = 0,\n                col_dim = 1):\n\n        super().__init__()\n\n        # weight matrices\n        \n        self.W_q = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False) \n        self.W_k = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False)\n        \n        self.W_v = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False)\n        \n\n        self.row_dim = row_dim\n        self.col_dim = col_dim\n        self.d_model = d_model\n        \n\n    \n    def forward(self , token_encodings , mask = None):\n        q = self.W_q(token_encodings)\n        k = self.W_k(token_encodings)\n        v = self.W_v(token_encodings)\n        \n        sims = torch.matmul(q , k.transpose(dim0 = self.row_dim,\n                                           dim1 = self.col_dim))\n        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n        # scaled_sims = sims / torch.tensor(self.d_model**0.5)\n\n        if mask is not None:\n            # mask = torch.tril(torch.ones(k.size(self.row_dim),k.size(self.row_dim)))\n            # mask = mask == 0\n            scaled_sims = scaled_sims.masked_fill(mask = mask,\n                                                 value = -1e-9)       \n\n        \n     \n\n        \n        attention_percents = F.softmax(scaled_sims , dim = self.col_dim)\n        attention_scores = torch.matmul(attention_percents , v)\n        \n        return attention_scores\n\n    \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:02:54.151452Z","iopub.execute_input":"2025-03-17T21:02:54.151788Z","iopub.status.idle":"2025-03-17T21:02:54.160612Z","shell.execute_reply.started":"2025-03-17T21:02:54.151753Z","shell.execute_reply":"2025-03-17T21:02:54.159143Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"encodings_matrix = torch.tensor([[1.16, 0.23],[0.57, 1.36],[4.41, -2.16]])\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:02:54.376319Z","iopub.execute_input":"2025-03-17T21:02:54.376762Z","iopub.status.idle":"2025-03-17T21:02:54.386431Z","shell.execute_reply.started":"2025-03-17T21:02:54.376713Z","shell.execute_reply":"2025-03-17T21:02:54.385334Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7eb0ce34abf0>"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"masked_self_attention = MaskedSelfAttention(d_model = 2,\n                              row_dim = 0,\n                              col_dim = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:02:54.595682Z","iopub.execute_input":"2025-03-17T21:02:54.596155Z","iopub.status.idle":"2025-03-17T21:02:54.604313Z","shell.execute_reply.started":"2025-03-17T21:02:54.596104Z","shell.execute_reply":"2025-03-17T21:02:54.602794Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"mask = torch.tril(torch.ones(3,3))\nmask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:02:54.757125Z","iopub.execute_input":"2025-03-17T21:02:54.757555Z","iopub.status.idle":"2025-03-17T21:02:54.765857Z","shell.execute_reply.started":"2025-03-17T21:02:54.757521Z","shell.execute_reply":"2025-03-17T21:02:54.764536Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"mask = mask == 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:02:54.909578Z","iopub.execute_input":"2025-03-17T21:02:54.909905Z","iopub.status.idle":"2025-03-17T21:02:54.914947Z","shell.execute_reply.started":"2025-03-17T21:02:54.909880Z","shell.execute_reply":"2025-03-17T21:02:54.913702Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:02:55.097381Z","iopub.execute_input":"2025-03-17T21:02:55.097776Z","iopub.status.idle":"2025-03-17T21:02:55.105658Z","shell.execute_reply.started":"2025-03-17T21:02:55.097750Z","shell.execute_reply":"2025-03-17T21:02:55.104389Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"masked_self_attention(encodings_matrix , mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:02:58.898560Z","iopub.execute_input":"2025-03-17T21:02:58.899014Z","iopub.status.idle":"2025-03-17T21:02:58.908124Z","shell.execute_reply.started":"2025-03-17T21:02:58.898957Z","shell.execute_reply":"2025-03-17T21:02:58.907025Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"tensor([[1.3921, 1.2440],\n        [1.2494, 1.1960],\n        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"},"metadata":{}}],"execution_count":44},{"cell_type":"markdown","source":"#### Encoder-Decoder Attention","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n\n    def __init__(self ,\n                 d_model = 2,\n                 row_dim = 0,\n                 col_dim = 1):\n\n        super().__init__()\n\n        # weight matrices\n        \n        self.W_q = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False) \n        self.W_k = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False)\n        \n        self.W_v = nn.Linear(in_features = d_model,\n                            out_features = d_model,\n                            bias = False)\n\n        self.row_dim = row_dim\n        self.col_dim = col_dim\n        self.d_model = d_model\n\n\n    \n    def forward(self , \n                encodings_q ,\n                encodings_k ,\n                encodings_v,\n               mask = None):\n        \n        q = self.W_q(encodings_q)\n        k = self.W_k(encodings_k)\n        v = self.W_v(encodings_v)\n        \n        sims = torch.matmul(q , k.transpose(dim0 = self.row_dim,\n                                           dim1 = self.col_dim))\n        \n        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n        # scaled_sims = sims / torch.tensor(self.d_model**0.5)\n\n        if mask is not None:\n            # mask = torch.tril(torch.ones(k.size(self.row_dim),k.size(self.row_dim)))\n            # mask = mask == 0\n            scaled_sims = scaled_sims.masked_fill(mask = mask,\n                                                 value = -1e-9)       \n       \n        attention_percents = F.softmax(scaled_sims , dim = self.col_dim)\n        attention_scores = torch.matmul(attention_percents , v)\n\n        return attention_scores\n\n    \nclass MultiHeadAttention(nn.Module):\n    \n    def __init__(self ,\n                 d_model = 2,\n                 row_dim = 0,\n                 col_dim = 1,\n                num_heads = 1):\n        super().__init__()\n\n\n        self.heads = nn.ModuleList(\n            [Attention(d_model = 2,row_dim = 0,col_dim = 1) \n             for _ in range(num_heads)]\n        )\n\n        self.col_dim = col_dim\n\n\n    def forward(self , \n                encodings_q ,\n                encodings_k ,\n                encodings_v):\n\n        return torch.cat(\n            [head(encodings_q ,\n                encodings_k ,\n                encodings_v)\n             for head in self.heads] , dim = self.col_dim\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:36:59.007046Z","iopub.execute_input":"2025-03-17T21:36:59.007522Z","iopub.status.idle":"2025-03-17T21:36:59.018667Z","shell.execute_reply.started":"2025-03-17T21:36:59.007487Z","shell.execute_reply":"2025-03-17T21:36:59.017348Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"encodings_q = torch.tensor([[1.3921, 1.2440],\n        [1.2494, 1.1960],\n        [3.4989, 2.2427]])\n\nencodings_k = torch.tensor([[1.3921, 1.2440],\n        [1.2494, 1.1960],\n        [3.4989, 2.2427]])\n\n\nencodings_v = torch.tensor([[1.3921, 1.2440],\n        [1.2494, 1.1960],\n        [3.4989, 2.2427]])\n\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:37:02.279261Z","iopub.execute_input":"2025-03-17T21:37:02.279590Z","iopub.status.idle":"2025-03-17T21:37:02.289480Z","shell.execute_reply.started":"2025-03-17T21:37:02.279565Z","shell.execute_reply":"2025-03-17T21:37:02.288349Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7eb0ce34abf0>"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"mha = MultiHeadAttention(d_model = 2,\n                 row_dim = 0,\n                 col_dim = 1,\n                num_heads = 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:37:29.483061Z","iopub.execute_input":"2025-03-17T21:37:29.483406Z","iopub.status.idle":"2025-03-17T21:37:29.490530Z","shell.execute_reply.started":"2025-03-17T21:37:29.483380Z","shell.execute_reply":"2025-03-17T21:37:29.489337Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"mha(encodings_q,encodings_q,encodings_q)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:37:34.162936Z","iopub.execute_input":"2025-03-17T21:37:34.163322Z","iopub.status.idle":"2025-03-17T21:37:34.178088Z","shell.execute_reply.started":"2025-03-17T21:37:34.163294Z","shell.execute_reply":"2025-03-17T21:37:34.176805Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.4082,  1.3614, -0.9187,  0.1218,  1.3103,  0.2117],\n        [ 0.4108,  1.3661, -0.9077,  0.1238,  1.3070,  0.2119],\n        [ 0.3628,  1.2806, -1.0814,  0.0919,  1.3522,  0.2101]],\n       grad_fn=<CatBackward0>)"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}